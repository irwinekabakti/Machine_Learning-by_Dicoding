# -*- coding: utf-8 -*-
"""covidSentiment171022.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ZftjTwNjqvHdg5Fhd0YXF31qc_Mb90a
"""

!chmod 600 /content/kaggle.json

! KAGGLE_CONFIG_DIR=/content/ kaggle datasets download -d datatattle/covid-19-nlp-text-classification

import zipfile
zip_file = zipfile.ZipFile('/content/covid-19-nlp-text-classification.zip')
zip_file.extractall('/tmp/')

import pandas as pd

df_train = pd.read_csv('/tmp/Corona_NLP_train.csv', encoding='latin-1')
df_train.head()

import pandas as pd

df_test = pd.read_csv('/tmp/Corona_NLP_test.csv', encoding='latin-1')
df_test.head()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split

!pip install tensorflow-addons
import tensorflow_addons as tfa
import re

!pip install tweet-preprocessor
import preprocessor as p
import string
import tensorflow as tf

from tensorflow.keras import Model
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Activation
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Embedding
from tensorflow.keras.layers import GlobalAveragePooling1D
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

plt.style.use("fivethirtyeight")

plt.figure(figsize=(16, 6))
location = sns.countplot(x= 'Location', data= df_train, hue="Sentiment", order=df_train.Location.value_counts()[:10].index)
location.set_title("Places which tweeted the most about COVID-19", y=1.05)

def axis_labels(ax):
    ax.set_ylabel("Number of tweets")
    ax.set_xlabel("")

axis_labels(location)

plt.show()

plt.figure(figsize=(6,6))

sentiments = df_train.Sentiment.value_counts()

sns.set_palette("rocket")
plt.pie(sentiments,
        labels= sentiments.index,
        autopct='%1.1f%%', startangle=80, 
        pctdistance=0.82, textprops={"fontsize": 14})

centreCircle = plt.Circle((0,0),0.65,fc='white')
fig = plt.gcf()
fig.gca().add_artist(centreCircle)

plt.tight_layout()
plt.title("Data Visulization")

plt.show()

ps = PorterStemmer()
df_train.head()

list_x = []
list_y = []

special_character = re.compile('[^0-9a-z #+_]')
special_symbol = re.compile('[/(){}\[\]\|@,;]')

stopping_words = set(stopwords.words('english'))

total_extreme_positive = 0
total_positive = 0
total_neutral = 0
total_negative = 0
total_extreme_negative = 0

pattern=r'[$+0-9]'

for _, data in df_train.iterrows():
  text = p.clean(data[4])
  text = text.split(' ')
  
  temp_str = ''
  
  for j in range(len(text)):
    temp_str += ' '+ps.stem(text[j])
  text = temp_str.strip()
  text = special_character.sub('', text)
  text = special_symbol.sub(' ', text)
  text = re.sub(pattern,'', text)
  text = ' '.join(word for word in text.split() if word not in stopping_words)

  list_x.append(text)
  new_str = ''

  if(data[5] == 'Positive'):
    total_positive += 1
  elif(data[5] == 'Negative'):
    total_negative += 1

  if(data[5] == 'Extremely Positive'):
    total_extreme_positive += 1
    new_str = 'Positive'
  elif(data[5] == 'Neutral'):
    total_neutral += 1
    new_str = data[5]
  elif(data[5] == 'Extremely Negative'):
    total_extreme_negative += 1
    new_str = 'Negative'
  else:
    new_str = data[5]

  list_y.append(new_str)
  
mixing_positive = total_extreme_positive + total_positive
mixing_negative = total_extreme_negative + total_negative

fig = plt.figure(figsize=(16, 4))
plt.bar(x=['Extremely Positive', 'Positive', 'Neutral', 'Negative', 'Extremely Negative'],
        height=[total_extreme_positive, total_positive, total_neutral, total_negative, total_extreme_negative],
        width=0.5)
plt.title('Data Distribution')
plt.show()

print(mixing_positive, total_neutral, mixing_negative)

result_positive = 0
result_neutral = 0
result_negative = 0

list_train = []
list_label = []

for i in range(len(list_x)):
  add_list = 0
  if( total_neutral > result_positive and list_y[i] == 'Positive'):
    add_list = 1
    result_positive += 1
  if(total_neutral  > result_negative and list_y[i] == 'Negative'):
    add_list = 1
    result_negative += 1
  if(total_neutral > result_neutral and list_y[i] == 'Neutral'):
    add_list = 1
    result_neutral += 1
  if(add_list == 1):
    list_train.append(list_x[i])
    list_label.append(list_y[i])

result_train = np.array(list_train)
result_label = np.array(list_label)

print(result_train.shape)
print(result_label.shape)

fig = plt.figure(figsize=(16, 4))
plt.bar(x=['Positive', 'Neutral', 'Negative'],
        height=[result_positive, result_neutral, result_negative],
        width=0.5)
plt.title('Data Distribution')
plt.show()

df_train.Location = df_train.Location.str.split(',').str[0]

df_train["Sentiment"] = df_train["Sentiment"].str.replace("Extremely Negative", "Negative")
df_train["Sentiment"] = df_train["Sentiment"].str.replace("Extremely Positive", "Positive")

df_test['Sentiment'] = df_test.Sentiment.str.replace('Extremely Positive', 'Positive')
df_test['Sentiment'] = df_test.Sentiment.str.replace('Extremely Negative', 'Negative')

plt.style.use("fivethirtyeight")

plt.figure(figsize=(16, 6))
location = sns.countplot(x= 'Location', data= df_train, hue="Sentiment", order=df_train.Location.value_counts()[:10].index)
location.set_title("Places which tweeted the most about COVID-19", y=1.05)

def axis_labels(ax):
    ax.set_ylabel("Number of tweets")
    ax.set_xlabel("")

axis_labels(location)

plt.show()

from sklearn.preprocessing import LabelEncoder

content_list = []

for i in range(len(result_train)):
  content_list.append(result_train[i].split(' '))

print(result_train)
print(result_label)

max_pad_len = 0
for i in range(len(content_list)):
  if(max_pad_len < len(content_list[i])):
    max_pad_len = len(content_list[i])

tokenizer = Tokenizer()
tokenizer.fit_on_texts(result_train) 

vocab_size = len(tokenizer.word_index) + 1
sekuens = tokenizer.texts_to_sequences(result_train)
padded = pad_sequences(sekuens, maxlen = max_pad_len, padding='post') 

labelencoder = LabelEncoder()
labelling = labelencoder.fit_transform(result_label)
label = tf.keras.utils.to_categorical(labelling)

x_train, x_test, y_train, y_test = train_test_split(padded, label, test_size=0.2, shuffle=True)

print(x_train)
print(y_train)

print(x_train.shape)

model = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, 64, input_length = max_pad_len),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Conv1D(128, 5, activation = 'relu'),
    
    tf.keras.layers.BatchNormalization(axis = 1),
    tf.keras.layers.MaxPooling1D(pool_size = 4),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout = 0.5, recurrent_dropout = 0.5)),
    
    tf.keras.layers.Dense(64),
    tf.keras.layers.Dropout(0.5),
    
    tf.keras.layers.Dense(32),
    tf.keras.layers.Dropout(0.5),

    tf.keras.layers.Dense(16),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(3, activation='softmax')])

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])
print(model.summary())

# callback
class historyCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs = {}):
    if(logs.get('val_accuracy') > 0.90 and logs.get('accuracy')>0.90):
      print('\n The Accuracy up to 90%, and forced to stop!')
      self.model.stop_training = True

callbacks = historyCallback()

history = model.fit(x_train, 
                    y_train,
                    validation_data = (x_test, y_test),
                    epochs = 30,
                    batch_size = 128,
                    verbose = 1,
                    callbacks = [callbacks])

acc = history.history['accuracy']
loss = history.history['loss']
validation_accuracy = history.history['val_accuracy']
validation_loss = history.history['val_loss']

epochs = range(len(acc))

plt.plot(epochs, acc, 'r', label='Training Accuracy')
plt.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')
plt.title('Graphics Accuracy Data Training and Data Validation')
plt.legend(loc=0)
plt.figure()
plt.show()

plt.plot(epochs, loss, 'r', label='Loss Training')
plt.plot(epochs, validation_loss, 'b', label='Loss Validation')
plt.title('Grapichs Loss Data Training and Data Validation')
plt.legend(loc=0)
plt.figure()
plt.show()